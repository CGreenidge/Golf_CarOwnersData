# -*- coding: utf-8 -*-
"""nationwidefinal

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J3WOgJruFKk8PvKyI-0oQBCnWHJVvnEM
"""

!pip install googletrans==4.0.0-rc1

import pandas as pd
from googletrans import Translator
import re

# Initialize Google Translator
translator = Translator()

# Load the Chinese CSV file with dtype specification for mixed-type columns
file_path = '/content/760k-Car-Owners-Nationwide-China-csv-2020.csv'

# Use low_memory=False to avoid memory fragmentation issues and explicitly specify dtype for mixed columns
df = pd.read_csv(file_path, dtype={'mail': 'str'}, low_memory=False)

# Translate the column headers and modify them (lowercase, replace spaces with underscores)
translated_columns = {}
for col in df.columns:
    translated = translator.translate(col, src='zh-cn', dest='en')
    translated_text = translated.text.lower().replace(' ', '_')
    translated_columns[col] = translated_text

# Rename columns with translated names
df.rename(columns=translated_columns, inplace=True)

# Split the DataFrame into 4 chunks
chunk_size = len(df) // 4
chunks = [df.iloc[i:i + chunk_size].copy() for i in range(0, len(df), chunk_size)]  # Using .copy() here

# List of columns to drop and move to the garbage file
columns_to_drop = ['gender', 'industry', 'monthly_salary', 'marriage', 'educate', 'brand', 'car', 'model', 'configuration', 'color', 'unnamed:_21']

# List to store cleaned and garbage data for merging later
cleaned_data_list = []
garbage_data_list = []

# Function to validate email addresses (assuming 'mail' as the column name)
def is_valid_email(email):
    if pd.isna(email) or email == 'NULL':
        return False
    # Regular expression for validating emails
    email_pattern = r'^[\w\.-]+@[\w\.-]+\.\w+$'
    return re.match(email_pattern, email) is not None

# Function to check for rows with 4 consecutive commas
def remove_consecutive_commas(chunk):
    # Create a mask for rows containing four consecutive commas
    consecutive_commas_mask = chunk.apply(lambda row: row.astype(str).str.contains(r',,,,', regex=True).any(), axis=1)
    return chunk[~consecutive_commas_mask], chunk[consecutive_commas_mask]

# Function to clean each chunk
def clean_chunk(chunk, chunk_index):
    garbage_data = pd.DataFrame()  # Empty DataFrame for garbage

    # Remove rows with 4 consecutive commas
    chunk, garbage_consecutive_commas = remove_consecutive_commas(chunk)
    garbage_data = pd.concat([garbage_data, garbage_consecutive_commas])

    # Drop only the columns that exist in the chunk
    columns_to_drop_in_chunk = [col for col in columns_to_drop if col in chunk.columns]

    if columns_to_drop_in_chunk:
        dropped_data = chunk[columns_to_drop_in_chunk]
        garbage_data = pd.concat([garbage_data, dropped_data])
        chunk = chunk.drop(columns=columns_to_drop_in_chunk)  # No inplace=True
    else:
        print(f"No columns to drop found in chunk {chunk_index}. Skipping drop operation.")

    # Check if 'mail' column exists before processing
    if 'mail' not in chunk.columns:
        print(f"'mail' column not found in chunk {chunk_index}. Skipping email validation.")
    else:
        # Identify rows with invalid email addresses
        invalid_email_mask = (chunk['mail'].str.contains('noemail@', na=False)) | ~chunk['mail'].apply(is_valid_email)
        garbage_email = chunk[invalid_email_mask]
        chunk = chunk[~invalid_email_mask]
        garbage_data = pd.concat([garbage_data, garbage_email])

    # Remove rows with missing or null important fields (adjust according to your columns)
    if {'id_card', 'mail'}.issubset(chunk.columns):
        missing_data = chunk[chunk[['id_card', 'mail']].isnull().any(axis=1)]
        chunk = chunk.dropna(subset=['id_card', 'mail'])
        garbage_data = pd.concat([garbage_data, missing_data])
    else:
        print(f"Required columns 'id_card' and/or 'mail' not found in chunk {chunk_index}. Skipping row validation.")

    # Drop duplicate rows based on 'id_card'
    if 'id_card' in chunk.columns:
        initial_length = len(chunk)
        chunk = chunk.drop_duplicates(subset='id_card')
        final_length = len(chunk)
        print(f"Chunk {chunk_index}: Dropped {initial_length - final_length} duplicate rows based on 'id_card'.")

    # Save the cleaned chunk and garbage data in lists for merging later
    cleaned_data_list.append(chunk)
    garbage_data_list.append(garbage_data)

# Clean each chunk and create a garbage file
for i, chunk in enumerate(chunks):
    clean_chunk(chunk, i + 1)

# Merge all cleaned data chunks together
if cleaned_data_list:
    merged_cleaned_data = pd.concat(cleaned_data_list, ignore_index=True)
    merged_cleaned_data.to_csv('/content/merged_cleaned_data.csv', index=False)
    print("Merged cleaned data saved to '/content/merged_cleaned_data.csv'.")

# Merge all garbage data chunks together
if garbage_data_list:
    merged_garbage_data = pd.concat(garbage_data_list, ignore_index=True)
    merged_garbage_data.to_csv('/content/merged_garbage_data.csv', index=False)
    print("Merged garbage data saved to '/content/merged_garbage_data.csv'.")

print("Cleaning, chunking, and merging completed!")

